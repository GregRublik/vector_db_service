# SERVICES:

Схема работы сервисов

```mermaid
graph TD
    B[VectorDBManager]
    B --> C[VectorStoreInterface]
    C --> D[FAISSStore]
    C --> E[ChromaStore]
    B --> F[EmbeddingFactory]
    F --> G[HuggingFaceEmbedding]
    
    a[API] <--> B
    
    a <--> b[bot-aiogram]
```

## BOT
Обработка пользовательских запросов, запросы отправляются к сервису векторной базы данных, получают необходимую 
информацию для контекста и передает ее в модель, получает ответ и отправляет его пользователю

## VECTORDB
БД может запускаться как на faiss, так и на chroma, при первом запуске данные из документов в папке docs чанкуются
и векторизируются закидываются в базу данных

У сервиса есть API для по которому можно получать данные по запросу 




# TODO:
- реализовать очередь для запросов
- подключить ретривер
- реализовать возможность добавления документов в базу данных
- автоматическое определение типа чанкования


3. нет никакой доки!
4. промпт менять. он относится к конкретному документу, а нам надо общий, чтобы мы подавали разные доки, разные темы и тд. 
5. я бы перешел к стандарты разработки на фаст апи со стандартными models.py / routers.py и тд...
Подход Рамиса выглядит более стандартиированным. Если и доделывать, то его реп. Кстати, у кого лучше работает бот?
6. Скармиливать ответ в ллм после того, как нашли. Чтобы ответ был более нативным, уважительным, персонализированным.


1. поменять промпты. попросить гпт ганписать более строгие промпты, предвариатлньо рассказав про систему
2. добавить логирование (можно langsmith) там до 5000 заросов в мес вроде бесплатно. Чтобы мы могли просматривать логироване из лк лангсмита. + можно кастомное логирование проводить в будущем. 
3. чанксайз, вероятно большеват, но надо тестировать на конкретноых документах
4. в доку дописать, расширить про каждый модуль, что есть что. + дописать команды по запуску и остановке и перезапуску контейнеров. Лучше сделать makefile, чтобы на будущее, при увеличении контейнеров и т д, мы все запускали в один клик. 
5. подумать над хранилищем документов (где потом будем хранить все доки)
6. посмотреть в сторону формирование эмбеддингов и реранжирования на модели 
Для формирования эмбеддингов: intfloat/multilingual-e5-large
Для реранжирования: BAAI/bge-reranker-v2-m3

Би-энкодеры нужны чтобы по тексту сформировать вектор. Затем эти вектора крайне быстро можно искать с помощью косинусного расстояния, например. Здесь важно что сравниваются уже не сами тексты, а некоторая функция от них, поэтому теряется часть информации. А вот кросс-энкодеры сразу сравнивают релевантность двух текстов, поэтому работают лучше, но по всей базе прогонять это намного дольше. Поэтому сначала отбирают не супер много кандидатов, а потом реранжируют

может прибавить точности гибридный поиск: сочетание индекса от энкодера и BM25. Его можно реализовать самостоятельно (объединяя поиски через langchain.EnsembleRetriever), а можно воспользоваться встроенным поиском специальных баз типа weaviate, milvus, qdrant.


для запуска выполняем команду 

    docker compose up --build
